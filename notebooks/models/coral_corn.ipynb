{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataset.creating_dataset import min_max_scale_data\n",
    "from dataset.splitting_dataset import split_dataframe\n",
    "from training.train_and_evaluate_models import train_and_evaluate_models\n",
    "\n",
    "\n",
    "PATH_TO_DATASET = os.path.join(\n",
    "    \"..\", \"..\", \"preprocessed_bestiaries\", \"bestiaries_full.csv\"\n",
    ")\n",
    "TEST_RESULT_FILE = os.path.join(\"results\", \"results_test_scenarios_probit.xlsx\")\n",
    "TRAIN_RESULT_FILE = os.path.join(\"results\", \"results_train_scenarios_probit.xlsx\")\n",
    "\n",
    "\n",
    "bestiaries = pd.read_csv(PATH_TO_DATASET, index_col=0)\n",
    "bestiaries = min_max_scale_data(bestiaries)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataframe(bestiaries)\n",
    "\n",
    "y_train += 1\n",
    "y_test += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T18:17:28.897896Z",
     "start_time": "2024-10-13T18:17:28.109597Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "X_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T18:17:18.413749Z",
     "start_time": "2024-10-13T18:17:18.388629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          str       dex       con       int       wis       cha        ac  \\\n",
       "428  0.529412  0.588235  0.428571  0.600000  0.470588  0.529412  0.358491   \n",
       "429  0.529412  0.411765  0.285714  0.400000  0.352941  0.470588  0.320755   \n",
       "430  0.470588  0.352941  0.428571  0.200000  0.294118  0.352941  0.283019   \n",
       "431  0.470588  0.529412  0.357143  0.333333  0.411765  0.294118  0.490566   \n",
       "432  0.470588  0.352941  0.142857  0.333333  0.529412  0.352941  0.339623   \n",
       "\n",
       "           hp  \n",
       "428  0.131886  \n",
       "429  0.048414  \n",
       "430  0.035058  \n",
       "431  0.257095  \n",
       "432  0.081803  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str</th>\n",
       "      <th>dex</th>\n",
       "      <th>con</th>\n",
       "      <th>int</th>\n",
       "      <th>wis</th>\n",
       "      <th>cha</th>\n",
       "      <th>ac</th>\n",
       "      <th>hp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.131886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.048414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.035058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.257095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.081803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 23\n",
    "\n",
    "# Other\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, feature_array, label_array, dtype=np.float32):\n",
    "\n",
    "        self.features = feature_array.astype(np.float32)\n",
    "        self.labels = label_array\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return inputs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([128, 50])\n",
      "Input label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = MyDataset(X_train.to_numpy(), y_train.to_numpy())\n",
    "test_dataset = MyDataset(X_test.to_numpy(), y_test.to_numpy())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # want to shuffle the dataset\n",
    "    num_workers=0,\n",
    ")  # number processes/CPUs to use\n",
    "\n",
    "\n",
    "# Checking the dataset\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input batch dimensions:\", inputs.shape)\n",
    "    print(\"Input label dimensions:\", labels.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "from torch import nn\n",
    "from coral_pytorch.layers import CoralLayer\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = CoralLayer(size_in=50, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "\n",
    "        ##### Use CORAL layer #####\n",
    "        logits = self.fc(x)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        return logits, probas\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "\n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = MLP(input_size=50, num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/011 | Loss: 2.1775\n",
      "Epoch: 002/100 | Batch 000/011 | Loss: 2.0583\n",
      "Epoch: 003/100 | Batch 000/011 | Loss: 2.1252\n",
      "Epoch: 004/100 | Batch 000/011 | Loss: 2.0616\n",
      "Epoch: 005/100 | Batch 000/011 | Loss: 1.9817\n",
      "Epoch: 006/100 | Batch 000/011 | Loss: 1.8627\n",
      "Epoch: 007/100 | Batch 000/011 | Loss: 1.8738\n",
      "Epoch: 008/100 | Batch 000/011 | Loss: 1.9805\n",
      "Epoch: 009/100 | Batch 000/011 | Loss: 1.7715\n",
      "Epoch: 010/100 | Batch 000/011 | Loss: 1.7385\n",
      "Epoch: 011/100 | Batch 000/011 | Loss: 1.8067\n",
      "Epoch: 012/100 | Batch 000/011 | Loss: 1.7782\n",
      "Epoch: 013/100 | Batch 000/011 | Loss: 1.8222\n",
      "Epoch: 014/100 | Batch 000/011 | Loss: 1.7732\n",
      "Epoch: 015/100 | Batch 000/011 | Loss: 1.7711\n",
      "Epoch: 016/100 | Batch 000/011 | Loss: 1.7409\n",
      "Epoch: 017/100 | Batch 000/011 | Loss: 1.5250\n",
      "Epoch: 018/100 | Batch 000/011 | Loss: 1.6339\n",
      "Epoch: 019/100 | Batch 000/011 | Loss: 1.7606\n",
      "Epoch: 020/100 | Batch 000/011 | Loss: 1.5905\n",
      "Epoch: 021/100 | Batch 000/011 | Loss: 1.6928\n",
      "Epoch: 022/100 | Batch 000/011 | Loss: 1.5012\n",
      "Epoch: 023/100 | Batch 000/011 | Loss: 1.6611\n",
      "Epoch: 024/100 | Batch 000/011 | Loss: 1.4984\n",
      "Epoch: 025/100 | Batch 000/011 | Loss: 1.4140\n",
      "Epoch: 026/100 | Batch 000/011 | Loss: 1.5253\n",
      "Epoch: 027/100 | Batch 000/011 | Loss: 1.5227\n",
      "Epoch: 028/100 | Batch 000/011 | Loss: 1.4362\n",
      "Epoch: 029/100 | Batch 000/011 | Loss: 1.3835\n",
      "Epoch: 030/100 | Batch 000/011 | Loss: 1.4006\n",
      "Epoch: 031/100 | Batch 000/011 | Loss: 1.3874\n",
      "Epoch: 032/100 | Batch 000/011 | Loss: 1.3022\n",
      "Epoch: 033/100 | Batch 000/011 | Loss: 1.3196\n",
      "Epoch: 034/100 | Batch 000/011 | Loss: 1.2997\n",
      "Epoch: 035/100 | Batch 000/011 | Loss: 1.2937\n",
      "Epoch: 036/100 | Batch 000/011 | Loss: 1.3717\n",
      "Epoch: 037/100 | Batch 000/011 | Loss: 1.2693\n",
      "Epoch: 038/100 | Batch 000/011 | Loss: 1.2902\n",
      "Epoch: 039/100 | Batch 000/011 | Loss: 1.2460\n",
      "Epoch: 040/100 | Batch 000/011 | Loss: 1.3429\n",
      "Epoch: 041/100 | Batch 000/011 | Loss: 1.3253\n",
      "Epoch: 042/100 | Batch 000/011 | Loss: 1.2653\n",
      "Epoch: 043/100 | Batch 000/011 | Loss: 1.3061\n",
      "Epoch: 044/100 | Batch 000/011 | Loss: 1.2172\n",
      "Epoch: 045/100 | Batch 000/011 | Loss: 1.2530\n",
      "Epoch: 046/100 | Batch 000/011 | Loss: 1.1226\n",
      "Epoch: 047/100 | Batch 000/011 | Loss: 1.2112\n",
      "Epoch: 048/100 | Batch 000/011 | Loss: 1.2230\n",
      "Epoch: 049/100 | Batch 000/011 | Loss: 1.1711\n",
      "Epoch: 050/100 | Batch 000/011 | Loss: 1.1527\n",
      "Epoch: 051/100 | Batch 000/011 | Loss: 1.1751\n",
      "Epoch: 052/100 | Batch 000/011 | Loss: 1.1208\n",
      "Epoch: 053/100 | Batch 000/011 | Loss: 1.1356\n",
      "Epoch: 054/100 | Batch 000/011 | Loss: 1.0112\n",
      "Epoch: 055/100 | Batch 000/011 | Loss: 1.1368\n",
      "Epoch: 056/100 | Batch 000/011 | Loss: 1.0914\n",
      "Epoch: 057/100 | Batch 000/011 | Loss: 1.0047\n",
      "Epoch: 058/100 | Batch 000/011 | Loss: 1.0362\n",
      "Epoch: 059/100 | Batch 000/011 | Loss: 1.0944\n",
      "Epoch: 060/100 | Batch 000/011 | Loss: 1.0856\n",
      "Epoch: 061/100 | Batch 000/011 | Loss: 1.0244\n",
      "Epoch: 062/100 | Batch 000/011 | Loss: 1.0854\n",
      "Epoch: 063/100 | Batch 000/011 | Loss: 1.0596\n",
      "Epoch: 064/100 | Batch 000/011 | Loss: 1.0404\n",
      "Epoch: 065/100 | Batch 000/011 | Loss: 1.1475\n",
      "Epoch: 066/100 | Batch 000/011 | Loss: 0.9906\n",
      "Epoch: 067/100 | Batch 000/011 | Loss: 1.0191\n",
      "Epoch: 068/100 | Batch 000/011 | Loss: 1.0245\n",
      "Epoch: 069/100 | Batch 000/011 | Loss: 1.0066\n",
      "Epoch: 070/100 | Batch 000/011 | Loss: 1.0550\n",
      "Epoch: 071/100 | Batch 000/011 | Loss: 0.9695\n",
      "Epoch: 072/100 | Batch 000/011 | Loss: 0.9807\n",
      "Epoch: 073/100 | Batch 000/011 | Loss: 1.0228\n",
      "Epoch: 074/100 | Batch 000/011 | Loss: 0.9901\n",
      "Epoch: 075/100 | Batch 000/011 | Loss: 0.9594\n",
      "Epoch: 076/100 | Batch 000/011 | Loss: 0.9338\n",
      "Epoch: 077/100 | Batch 000/011 | Loss: 0.9633\n",
      "Epoch: 078/100 | Batch 000/011 | Loss: 0.9513\n",
      "Epoch: 079/100 | Batch 000/011 | Loss: 0.9231\n",
      "Epoch: 080/100 | Batch 000/011 | Loss: 0.9177\n",
      "Epoch: 081/100 | Batch 000/011 | Loss: 0.9698\n",
      "Epoch: 082/100 | Batch 000/011 | Loss: 1.0047\n",
      "Epoch: 083/100 | Batch 000/011 | Loss: 1.1009\n",
      "Epoch: 084/100 | Batch 000/011 | Loss: 1.0306\n",
      "Epoch: 085/100 | Batch 000/011 | Loss: 0.9274\n",
      "Epoch: 086/100 | Batch 000/011 | Loss: 0.9923\n",
      "Epoch: 087/100 | Batch 000/011 | Loss: 0.9147\n",
      "Epoch: 088/100 | Batch 000/011 | Loss: 0.9045\n",
      "Epoch: 089/100 | Batch 000/011 | Loss: 0.9113\n",
      "Epoch: 090/100 | Batch 000/011 | Loss: 0.8693\n",
      "Epoch: 091/100 | Batch 000/011 | Loss: 0.8360\n",
      "Epoch: 092/100 | Batch 000/011 | Loss: 0.8381\n",
      "Epoch: 093/100 | Batch 000/011 | Loss: 0.8854\n",
      "Epoch: 094/100 | Batch 000/011 | Loss: 0.8532\n",
      "Epoch: 095/100 | Batch 000/011 | Loss: 0.8612\n",
      "Epoch: 096/100 | Batch 000/011 | Loss: 0.8073\n",
      "Epoch: 097/100 | Batch 000/011 | Loss: 0.8755\n",
      "Epoch: 098/100 | Batch 000/011 | Loss: 0.8556\n",
      "Epoch: 099/100 | Batch 000/011 | Loss: 0.8934\n",
      "Epoch: 100/100 | Batch 000/011 | Loss: 0.8665\n"
     ]
    }
   ],
   "source": [
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.losses import coral_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        ##### Convert class labels for CORAL\n",
    "        levels = levels_from_labelbatch(class_labels + 1, num_classes=NUM_CLASSES)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        levels = levels.to(DEVICE)\n",
    "        logits, probas = model(features)\n",
    "\n",
    "        #### CORAL loss\n",
    "        loss = coral_loss(logits, levels)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print(\n",
    "                \"Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f\"\n",
    "                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "\n",
    "def compute_mae_and_mse(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mae, mse, acc, num_examples = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits, probas = model(features)\n",
    "            predicted_labels = proba_to_label(probas).float()\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "            mse += torch.sum((predicted_labels - targets) ** 2)\n",
    "\n",
    "        mae = mae / num_examples\n",
    "        mse = mse / num_examples\n",
    "        return mae, mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\n",
    "test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (train/test): 0.92 | 0.85\n",
      "Mean squared error (train/test): 0.93 | 1.07\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}\")\n",
    "print(f\"Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Other\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden_1=300, num_hidden_2=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.my_network = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            ### Specify CORN layer\n",
    "            torch.nn.Linear(50, (num_classes - 1))\n",
    "            ###--------------------------------------------------------------------###\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.my_network(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = MLP(input_size=50, num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/011 | Cost: 0.2115\n",
      "Epoch: 002/100 | Batch 000/011 | Cost: 0.1162\n",
      "Epoch: 003/100 | Batch 000/011 | Cost: 0.1362\n",
      "Epoch: 004/100 | Batch 000/011 | Cost: 0.1631\n",
      "Epoch: 005/100 | Batch 000/011 | Cost: 0.1534\n",
      "Epoch: 006/100 | Batch 000/011 | Cost: 0.1528\n",
      "Epoch: 007/100 | Batch 000/011 | Cost: 0.1057\n",
      "Epoch: 008/100 | Batch 000/011 | Cost: 0.1110\n",
      "Epoch: 009/100 | Batch 000/011 | Cost: 0.1342\n",
      "Epoch: 010/100 | Batch 000/011 | Cost: 0.0992\n",
      "Epoch: 011/100 | Batch 000/011 | Cost: 0.1270\n",
      "Epoch: 012/100 | Batch 000/011 | Cost: 0.0764\n",
      "Epoch: 013/100 | Batch 000/011 | Cost: 0.0928\n",
      "Epoch: 014/100 | Batch 000/011 | Cost: 0.0637\n",
      "Epoch: 015/100 | Batch 000/011 | Cost: 0.1006\n",
      "Epoch: 016/100 | Batch 000/011 | Cost: 0.0764\n",
      "Epoch: 017/100 | Batch 000/011 | Cost: 0.0971\n",
      "Epoch: 018/100 | Batch 000/011 | Cost: 0.1042\n",
      "Epoch: 019/100 | Batch 000/011 | Cost: 0.0603\n",
      "Epoch: 020/100 | Batch 000/011 | Cost: 0.0797\n",
      "Epoch: 021/100 | Batch 000/011 | Cost: 0.0916\n",
      "Epoch: 022/100 | Batch 000/011 | Cost: 0.0773\n",
      "Epoch: 023/100 | Batch 000/011 | Cost: 0.0727\n",
      "Epoch: 024/100 | Batch 000/011 | Cost: 0.0645\n",
      "Epoch: 025/100 | Batch 000/011 | Cost: 0.0683\n",
      "Epoch: 026/100 | Batch 000/011 | Cost: 0.0681\n",
      "Epoch: 027/100 | Batch 000/011 | Cost: 0.0520\n",
      "Epoch: 028/100 | Batch 000/011 | Cost: 0.0650\n",
      "Epoch: 029/100 | Batch 000/011 | Cost: 0.0579\n",
      "Epoch: 030/100 | Batch 000/011 | Cost: 0.0530\n",
      "Epoch: 031/100 | Batch 000/011 | Cost: 0.0601\n",
      "Epoch: 032/100 | Batch 000/011 | Cost: 0.0507\n",
      "Epoch: 033/100 | Batch 000/011 | Cost: 0.0536\n",
      "Epoch: 034/100 | Batch 000/011 | Cost: 0.0466\n",
      "Epoch: 035/100 | Batch 000/011 | Cost: 0.0706\n",
      "Epoch: 036/100 | Batch 000/011 | Cost: 0.0425\n",
      "Epoch: 037/100 | Batch 000/011 | Cost: 0.0316\n",
      "Epoch: 038/100 | Batch 000/011 | Cost: 0.0469\n",
      "Epoch: 039/100 | Batch 000/011 | Cost: 0.0400\n",
      "Epoch: 040/100 | Batch 000/011 | Cost: 0.0401\n",
      "Epoch: 041/100 | Batch 000/011 | Cost: 0.0440\n",
      "Epoch: 042/100 | Batch 000/011 | Cost: 0.0367\n",
      "Epoch: 043/100 | Batch 000/011 | Cost: 0.0469\n",
      "Epoch: 044/100 | Batch 000/011 | Cost: 0.0572\n",
      "Epoch: 045/100 | Batch 000/011 | Cost: 0.0414\n",
      "Epoch: 046/100 | Batch 000/011 | Cost: 0.0646\n",
      "Epoch: 047/100 | Batch 000/011 | Cost: 0.0346\n",
      "Epoch: 048/100 | Batch 000/011 | Cost: 0.0478\n",
      "Epoch: 049/100 | Batch 000/011 | Cost: 0.0484\n",
      "Epoch: 050/100 | Batch 000/011 | Cost: 0.0517\n",
      "Epoch: 051/100 | Batch 000/011 | Cost: 0.0239\n",
      "Epoch: 052/100 | Batch 000/011 | Cost: 0.0379\n",
      "Epoch: 053/100 | Batch 000/011 | Cost: 0.0586\n",
      "Epoch: 054/100 | Batch 000/011 | Cost: 0.0429\n",
      "Epoch: 055/100 | Batch 000/011 | Cost: 0.0425\n",
      "Epoch: 056/100 | Batch 000/011 | Cost: 0.0296\n",
      "Epoch: 057/100 | Batch 000/011 | Cost: 0.0432\n",
      "Epoch: 058/100 | Batch 000/011 | Cost: 0.0322\n",
      "Epoch: 059/100 | Batch 000/011 | Cost: 0.0352\n",
      "Epoch: 060/100 | Batch 000/011 | Cost: 0.0382\n",
      "Epoch: 061/100 | Batch 000/011 | Cost: 0.0515\n",
      "Epoch: 062/100 | Batch 000/011 | Cost: 0.0320\n",
      "Epoch: 063/100 | Batch 000/011 | Cost: 0.0283\n",
      "Epoch: 064/100 | Batch 000/011 | Cost: 0.0307\n",
      "Epoch: 065/100 | Batch 000/011 | Cost: 0.0317\n",
      "Epoch: 066/100 | Batch 000/011 | Cost: 0.0323\n",
      "Epoch: 067/100 | Batch 000/011 | Cost: 0.0351\n",
      "Epoch: 068/100 | Batch 000/011 | Cost: 0.0220\n",
      "Epoch: 069/100 | Batch 000/011 | Cost: 0.0355\n",
      "Epoch: 070/100 | Batch 000/011 | Cost: 0.0328\n",
      "Epoch: 071/100 | Batch 000/011 | Cost: 0.0431\n",
      "Epoch: 072/100 | Batch 000/011 | Cost: 0.0438\n",
      "Epoch: 073/100 | Batch 000/011 | Cost: 0.0342\n",
      "Epoch: 074/100 | Batch 000/011 | Cost: 0.0263\n",
      "Epoch: 075/100 | Batch 000/011 | Cost: 0.0314\n",
      "Epoch: 076/100 | Batch 000/011 | Cost: 0.0298\n",
      "Epoch: 077/100 | Batch 000/011 | Cost: 0.0232\n",
      "Epoch: 078/100 | Batch 000/011 | Cost: 0.0168\n",
      "Epoch: 079/100 | Batch 000/011 | Cost: 0.0261\n",
      "Epoch: 080/100 | Batch 000/011 | Cost: 0.0352\n",
      "Epoch: 081/100 | Batch 000/011 | Cost: 0.0237\n",
      "Epoch: 082/100 | Batch 000/011 | Cost: 0.0551\n",
      "Epoch: 083/100 | Batch 000/011 | Cost: 0.0213\n",
      "Epoch: 084/100 | Batch 000/011 | Cost: 0.0225\n",
      "Epoch: 085/100 | Batch 000/011 | Cost: 0.0329\n",
      "Epoch: 086/100 | Batch 000/011 | Cost: 0.0502\n",
      "Epoch: 087/100 | Batch 000/011 | Cost: 0.0455\n",
      "Epoch: 088/100 | Batch 000/011 | Cost: 0.0213\n",
      "Epoch: 089/100 | Batch 000/011 | Cost: 0.0347\n",
      "Epoch: 090/100 | Batch 000/011 | Cost: 0.0277\n",
      "Epoch: 091/100 | Batch 000/011 | Cost: 0.0347\n",
      "Epoch: 092/100 | Batch 000/011 | Cost: 0.0162\n",
      "Epoch: 093/100 | Batch 000/011 | Cost: 0.0258\n",
      "Epoch: 094/100 | Batch 000/011 | Cost: 0.0196\n",
      "Epoch: 095/100 | Batch 000/011 | Cost: 0.0243\n",
      "Epoch: 096/100 | Batch 000/011 | Cost: 0.0256\n",
      "Epoch: 097/100 | Batch 000/011 | Cost: 0.0214\n",
      "Epoch: 098/100 | Batch 000/011 | Cost: 0.0206\n",
      "Epoch: 099/100 | Batch 000/011 | Cost: 0.0195\n",
      "Epoch: 100/100 | Batch 000/011 | Cost: 0.0175\n"
     ]
    }
   ],
   "source": [
    "from coral_pytorch.losses import corn_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        class_labels = class_labels.to(DEVICE)\n",
    "        features = features.to(DEVICE)\n",
    "        logits = model(features)\n",
    "\n",
    "        #### CORN loss\n",
    "        loss = corn_loss(logits, class_labels + 1, NUM_CLASSES)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print(\n",
    "                \"Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f\"\n",
    "                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "\n",
    "\n",
    "def compute_mae_and_mse(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mae, mse, acc, num_examples = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            predicted_labels = corn_label_from_logits(logits).float()\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "            mse += torch.sum((predicted_labels - targets) ** 2)\n",
    "\n",
    "        mae = mae / num_examples\n",
    "        mse = mse / num_examples\n",
    "        return mae, mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (train/test): 3.95 | 6.68\n",
      "Mean squared error (train/test): 34.77 | 69.69\n"
     ]
    }
   ],
   "source": [
    "train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\n",
    "test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n",
    "print(f\"Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}\")\n",
    "print(f\"Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
