{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        str       dex       con       int       wis       cha        ac  \\\n0  0.588235  0.588235  0.357143  0.200000  0.470588  0.529412  0.509434   \n1  0.588235  0.411765  0.428571  0.600000  0.411765  0.411765  0.396226   \n2  0.529412  0.470588  0.214286  0.400000  0.470588  0.352941  0.320755   \n3  0.352941  0.529412  0.214286  0.333333  0.470588  0.352941  0.358491   \n4  0.588235  0.588235  0.500000  0.466667  0.529412  0.588235  0.490566   \n\n         hp  perception  fortitude  ...  area-damage_weakness  cold_weakness  \\\n0  0.252087    0.156522   0.425532  ...                   0.0            0.0   \n1  0.123539    0.130435   0.297872  ...                   0.0            0.0   \n2  0.078464    0.069565   0.170213  ...                   0.0            0.0   \n3  0.048414    0.104348   0.170213  ...                   0.0            0.0   \n4  0.198664    0.156522   0.361702  ...                   0.0            0.0   \n\n   cold-iron_weakness  evil_weakness  fire_weakness  good_weakness  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n\n   slashing_weakness  splash-damage_weakness  \\\n0                0.0                     0.0   \n1                0.0                     0.0   \n2                0.0                     0.0   \n3                0.0                     0.0   \n4                0.0                     0.0   \n\n                                   book  level  \n0   Pathfinder #164: Hands of the Devil      8  \n1  Pathfinder #163: Ruins of Gauntlight      5  \n2  Pathfinder #163: Ruins of Gauntlight      3  \n3   Pathfinder #164: Hands of the Devil      3  \n4   Pathfinder #164: Hands of the Devil      8  \n\n[5 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>str</th>\n      <th>dex</th>\n      <th>con</th>\n      <th>int</th>\n      <th>wis</th>\n      <th>cha</th>\n      <th>ac</th>\n      <th>hp</th>\n      <th>perception</th>\n      <th>fortitude</th>\n      <th>...</th>\n      <th>area-damage_weakness</th>\n      <th>cold_weakness</th>\n      <th>cold-iron_weakness</th>\n      <th>evil_weakness</th>\n      <th>fire_weakness</th>\n      <th>good_weakness</th>\n      <th>slashing_weakness</th>\n      <th>splash-damage_weakness</th>\n      <th>book</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.588235</td>\n      <td>0.588235</td>\n      <td>0.357143</td>\n      <td>0.200000</td>\n      <td>0.470588</td>\n      <td>0.529412</td>\n      <td>0.509434</td>\n      <td>0.252087</td>\n      <td>0.156522</td>\n      <td>0.425532</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Pathfinder #164: Hands of the Devil</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.588235</td>\n      <td>0.411765</td>\n      <td>0.428571</td>\n      <td>0.600000</td>\n      <td>0.411765</td>\n      <td>0.411765</td>\n      <td>0.396226</td>\n      <td>0.123539</td>\n      <td>0.130435</td>\n      <td>0.297872</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Pathfinder #163: Ruins of Gauntlight</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.529412</td>\n      <td>0.470588</td>\n      <td>0.214286</td>\n      <td>0.400000</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.320755</td>\n      <td>0.078464</td>\n      <td>0.069565</td>\n      <td>0.170213</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Pathfinder #163: Ruins of Gauntlight</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.352941</td>\n      <td>0.529412</td>\n      <td>0.214286</td>\n      <td>0.333333</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.358491</td>\n      <td>0.048414</td>\n      <td>0.104348</td>\n      <td>0.170213</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Pathfinder #164: Hands of the Devil</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.588235</td>\n      <td>0.588235</td>\n      <td>0.500000</td>\n      <td>0.466667</td>\n      <td>0.529412</td>\n      <td>0.588235</td>\n      <td>0.490566</td>\n      <td>0.198664</td>\n      <td>0.156522</td>\n      <td>0.361702</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Pathfinder #164: Hands of the Devil</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 52 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from training.creating_dataset import load_and_preprocess_data, min_max_scale_data\n",
    "from training.constants import FEATURES, DATASET_FILES, ORDERED_CHARACTERISTICS_FULL\n",
    "\n",
    "\n",
    "bestiaries = load_and_preprocess_data(\n",
    "    [f\"../../pathfinder_2e_remaster_data/{f}\" for f in DATASET_FILES], FEATURES\n",
    ")\n",
    "\n",
    "bestiaries = min_max_scale_data(bestiaries)\n",
    "bestiaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from training.splitting_dataset import split_dataframe, get_date_books_mapping\n",
    "\n",
    "os.chdir(\"../../training\")\n",
    "books_dates_map = get_date_books_mapping()\n",
    "\n",
    "books_to_include = [\n",
    "    book for _, row in books_dates_map[\"books\"].iteritems() for book in row\n",
    "]\n",
    "bestiaries = bestiaries[bestiaries[\"book\"].isin(books_to_include)]\n",
    "X_train, X_test, y_train, y_test = split_dataframe(bestiaries)\n",
    "os.chdir(\"../notebooks/models\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "          str       dex       con       int       wis       cha        ac  \\\n651  0.588235  0.470588  0.428571  0.000000  0.411765  0.176471  0.452830   \n652  0.000000  0.705882  0.357143  0.666667  0.588235  0.705882  0.547170   \n656  0.411765  0.588235  0.285714  0.400000  0.411765  0.588235  0.396226   \n657  0.588235  0.647059  0.428571  0.533333  0.411765  0.529412  0.490566   \n659  0.647059  0.470588  0.428571  0.466667  0.647059  0.588235  0.490566   \n\n           hp  perception  fortitude  ...  poison_resistance  \\\n651  0.148581    0.104348   0.340426  ...                0.0   \n652  0.290484    0.191304   0.382979  ...                0.0   \n656  0.081803    0.095652   0.191489  ...                0.0   \n657  0.207012    0.139130   0.340426  ...                0.0   \n659  0.273790    0.156522   0.297872  ...                0.0   \n\n     slashing_resistance  area-damage_weakness  cold_weakness  \\\n651                 0.25                   0.0            0.0   \n652                 0.00                   0.0            0.0   \n656                 0.25                   0.0            0.0   \n657                 0.00                   0.0            0.0   \n659                 0.00                   0.0            0.0   \n\n     cold-iron_weakness  evil_weakness  fire_weakness  good_weakness  \\\n651                0.00            0.0            0.0            0.0   \n652                0.25            0.0            0.0            0.0   \n656                0.00            0.0            0.0            0.0   \n657                0.00            0.0            0.0            0.0   \n659                0.00            0.0            0.0            0.0   \n\n     slashing_weakness  splash-damage_weakness  \n651                0.0                     0.0  \n652                0.0                     0.0  \n656                0.0                     0.0  \n657                0.0                     0.0  \n659                0.0                     0.0  \n\n[5 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>str</th>\n      <th>dex</th>\n      <th>con</th>\n      <th>int</th>\n      <th>wis</th>\n      <th>cha</th>\n      <th>ac</th>\n      <th>hp</th>\n      <th>perception</th>\n      <th>fortitude</th>\n      <th>...</th>\n      <th>poison_resistance</th>\n      <th>slashing_resistance</th>\n      <th>area-damage_weakness</th>\n      <th>cold_weakness</th>\n      <th>cold-iron_weakness</th>\n      <th>evil_weakness</th>\n      <th>fire_weakness</th>\n      <th>good_weakness</th>\n      <th>slashing_weakness</th>\n      <th>splash-damage_weakness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>651</th>\n      <td>0.588235</td>\n      <td>0.470588</td>\n      <td>0.428571</td>\n      <td>0.000000</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.452830</td>\n      <td>0.148581</td>\n      <td>0.104348</td>\n      <td>0.340426</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>652</th>\n      <td>0.000000</td>\n      <td>0.705882</td>\n      <td>0.357143</td>\n      <td>0.666667</td>\n      <td>0.588235</td>\n      <td>0.705882</td>\n      <td>0.547170</td>\n      <td>0.290484</td>\n      <td>0.191304</td>\n      <td>0.382979</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>656</th>\n      <td>0.411765</td>\n      <td>0.588235</td>\n      <td>0.285714</td>\n      <td>0.400000</td>\n      <td>0.411765</td>\n      <td>0.588235</td>\n      <td>0.396226</td>\n      <td>0.081803</td>\n      <td>0.095652</td>\n      <td>0.191489</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>657</th>\n      <td>0.588235</td>\n      <td>0.647059</td>\n      <td>0.428571</td>\n      <td>0.533333</td>\n      <td>0.411765</td>\n      <td>0.529412</td>\n      <td>0.490566</td>\n      <td>0.207012</td>\n      <td>0.139130</td>\n      <td>0.340426</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>0.647059</td>\n      <td>0.470588</td>\n      <td>0.428571</td>\n      <td>0.466667</td>\n      <td>0.647059</td>\n      <td>0.588235</td>\n      <td>0.490566</td>\n      <td>0.273790</td>\n      <td>0.156522</td>\n      <td>0.297872</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 50 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 23\n",
    "\n",
    "# Other\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, feature_array, label_array, dtype=np.float32):\n",
    "\n",
    "        self.features = feature_array.astype(np.float32)\n",
    "        self.labels = label_array\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return inputs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([128, 50])\n",
      "Input label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = MyDataset(X_train.to_numpy(), y_train.to_numpy())\n",
    "test_dataset = MyDataset(X_test.to_numpy(), y_test.to_numpy())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # want to shuffle the dataset\n",
    "    num_workers=0,\n",
    ")  # number processes/CPUs to use\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Checking the dataset\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input batch dimensions:\", inputs.shape)\n",
    "    print(\"Input label dimensions:\", labels.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "from torch import nn\n",
    "from coral_pytorch.layers import CoralLayer\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = CoralLayer(size_in=50, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "\n",
    "        ##### Use CORAL layer #####\n",
    "        logits = self.fc(x)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        return logits, probas\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "\n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = MLP(input_size=50, num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/011 | Loss: 2.1775\n",
      "Epoch: 002/100 | Batch 000/011 | Loss: 2.0583\n",
      "Epoch: 003/100 | Batch 000/011 | Loss: 2.1252\n",
      "Epoch: 004/100 | Batch 000/011 | Loss: 2.0616\n",
      "Epoch: 005/100 | Batch 000/011 | Loss: 1.9817\n",
      "Epoch: 006/100 | Batch 000/011 | Loss: 1.8627\n",
      "Epoch: 007/100 | Batch 000/011 | Loss: 1.8738\n",
      "Epoch: 008/100 | Batch 000/011 | Loss: 1.9805\n",
      "Epoch: 009/100 | Batch 000/011 | Loss: 1.7715\n",
      "Epoch: 010/100 | Batch 000/011 | Loss: 1.7385\n",
      "Epoch: 011/100 | Batch 000/011 | Loss: 1.8067\n",
      "Epoch: 012/100 | Batch 000/011 | Loss: 1.7782\n",
      "Epoch: 013/100 | Batch 000/011 | Loss: 1.8222\n",
      "Epoch: 014/100 | Batch 000/011 | Loss: 1.7732\n",
      "Epoch: 015/100 | Batch 000/011 | Loss: 1.7711\n",
      "Epoch: 016/100 | Batch 000/011 | Loss: 1.7409\n",
      "Epoch: 017/100 | Batch 000/011 | Loss: 1.5250\n",
      "Epoch: 018/100 | Batch 000/011 | Loss: 1.6339\n",
      "Epoch: 019/100 | Batch 000/011 | Loss: 1.7606\n",
      "Epoch: 020/100 | Batch 000/011 | Loss: 1.5905\n",
      "Epoch: 021/100 | Batch 000/011 | Loss: 1.6928\n",
      "Epoch: 022/100 | Batch 000/011 | Loss: 1.5012\n",
      "Epoch: 023/100 | Batch 000/011 | Loss: 1.6611\n",
      "Epoch: 024/100 | Batch 000/011 | Loss: 1.4984\n",
      "Epoch: 025/100 | Batch 000/011 | Loss: 1.4140\n",
      "Epoch: 026/100 | Batch 000/011 | Loss: 1.5253\n",
      "Epoch: 027/100 | Batch 000/011 | Loss: 1.5227\n",
      "Epoch: 028/100 | Batch 000/011 | Loss: 1.4362\n",
      "Epoch: 029/100 | Batch 000/011 | Loss: 1.3835\n",
      "Epoch: 030/100 | Batch 000/011 | Loss: 1.4006\n",
      "Epoch: 031/100 | Batch 000/011 | Loss: 1.3874\n",
      "Epoch: 032/100 | Batch 000/011 | Loss: 1.3022\n",
      "Epoch: 033/100 | Batch 000/011 | Loss: 1.3196\n",
      "Epoch: 034/100 | Batch 000/011 | Loss: 1.2997\n",
      "Epoch: 035/100 | Batch 000/011 | Loss: 1.2937\n",
      "Epoch: 036/100 | Batch 000/011 | Loss: 1.3717\n",
      "Epoch: 037/100 | Batch 000/011 | Loss: 1.2693\n",
      "Epoch: 038/100 | Batch 000/011 | Loss: 1.2902\n",
      "Epoch: 039/100 | Batch 000/011 | Loss: 1.2460\n",
      "Epoch: 040/100 | Batch 000/011 | Loss: 1.3429\n",
      "Epoch: 041/100 | Batch 000/011 | Loss: 1.3253\n",
      "Epoch: 042/100 | Batch 000/011 | Loss: 1.2653\n",
      "Epoch: 043/100 | Batch 000/011 | Loss: 1.3061\n",
      "Epoch: 044/100 | Batch 000/011 | Loss: 1.2172\n",
      "Epoch: 045/100 | Batch 000/011 | Loss: 1.2530\n",
      "Epoch: 046/100 | Batch 000/011 | Loss: 1.1226\n",
      "Epoch: 047/100 | Batch 000/011 | Loss: 1.2112\n",
      "Epoch: 048/100 | Batch 000/011 | Loss: 1.2230\n",
      "Epoch: 049/100 | Batch 000/011 | Loss: 1.1711\n",
      "Epoch: 050/100 | Batch 000/011 | Loss: 1.1527\n",
      "Epoch: 051/100 | Batch 000/011 | Loss: 1.1751\n",
      "Epoch: 052/100 | Batch 000/011 | Loss: 1.1208\n",
      "Epoch: 053/100 | Batch 000/011 | Loss: 1.1356\n",
      "Epoch: 054/100 | Batch 000/011 | Loss: 1.0112\n",
      "Epoch: 055/100 | Batch 000/011 | Loss: 1.1368\n",
      "Epoch: 056/100 | Batch 000/011 | Loss: 1.0914\n",
      "Epoch: 057/100 | Batch 000/011 | Loss: 1.0047\n",
      "Epoch: 058/100 | Batch 000/011 | Loss: 1.0362\n",
      "Epoch: 059/100 | Batch 000/011 | Loss: 1.0944\n",
      "Epoch: 060/100 | Batch 000/011 | Loss: 1.0856\n",
      "Epoch: 061/100 | Batch 000/011 | Loss: 1.0244\n",
      "Epoch: 062/100 | Batch 000/011 | Loss: 1.0854\n",
      "Epoch: 063/100 | Batch 000/011 | Loss: 1.0596\n",
      "Epoch: 064/100 | Batch 000/011 | Loss: 1.0404\n",
      "Epoch: 065/100 | Batch 000/011 | Loss: 1.1475\n",
      "Epoch: 066/100 | Batch 000/011 | Loss: 0.9906\n",
      "Epoch: 067/100 | Batch 000/011 | Loss: 1.0191\n",
      "Epoch: 068/100 | Batch 000/011 | Loss: 1.0245\n",
      "Epoch: 069/100 | Batch 000/011 | Loss: 1.0066\n",
      "Epoch: 070/100 | Batch 000/011 | Loss: 1.0550\n",
      "Epoch: 071/100 | Batch 000/011 | Loss: 0.9695\n",
      "Epoch: 072/100 | Batch 000/011 | Loss: 0.9807\n",
      "Epoch: 073/100 | Batch 000/011 | Loss: 1.0228\n",
      "Epoch: 074/100 | Batch 000/011 | Loss: 0.9901\n",
      "Epoch: 075/100 | Batch 000/011 | Loss: 0.9594\n",
      "Epoch: 076/100 | Batch 000/011 | Loss: 0.9338\n",
      "Epoch: 077/100 | Batch 000/011 | Loss: 0.9633\n",
      "Epoch: 078/100 | Batch 000/011 | Loss: 0.9513\n",
      "Epoch: 079/100 | Batch 000/011 | Loss: 0.9231\n",
      "Epoch: 080/100 | Batch 000/011 | Loss: 0.9177\n",
      "Epoch: 081/100 | Batch 000/011 | Loss: 0.9698\n",
      "Epoch: 082/100 | Batch 000/011 | Loss: 1.0047\n",
      "Epoch: 083/100 | Batch 000/011 | Loss: 1.1009\n",
      "Epoch: 084/100 | Batch 000/011 | Loss: 1.0306\n",
      "Epoch: 085/100 | Batch 000/011 | Loss: 0.9274\n",
      "Epoch: 086/100 | Batch 000/011 | Loss: 0.9923\n",
      "Epoch: 087/100 | Batch 000/011 | Loss: 0.9147\n",
      "Epoch: 088/100 | Batch 000/011 | Loss: 0.9045\n",
      "Epoch: 089/100 | Batch 000/011 | Loss: 0.9113\n",
      "Epoch: 090/100 | Batch 000/011 | Loss: 0.8693\n",
      "Epoch: 091/100 | Batch 000/011 | Loss: 0.8360\n",
      "Epoch: 092/100 | Batch 000/011 | Loss: 0.8381\n",
      "Epoch: 093/100 | Batch 000/011 | Loss: 0.8854\n",
      "Epoch: 094/100 | Batch 000/011 | Loss: 0.8532\n",
      "Epoch: 095/100 | Batch 000/011 | Loss: 0.8612\n",
      "Epoch: 096/100 | Batch 000/011 | Loss: 0.8073\n",
      "Epoch: 097/100 | Batch 000/011 | Loss: 0.8755\n",
      "Epoch: 098/100 | Batch 000/011 | Loss: 0.8556\n",
      "Epoch: 099/100 | Batch 000/011 | Loss: 0.8934\n",
      "Epoch: 100/100 | Batch 000/011 | Loss: 0.8665\n"
     ]
    }
   ],
   "source": [
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.losses import coral_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        ##### Convert class labels for CORAL\n",
    "        levels = levels_from_labelbatch(class_labels + 1, num_classes=NUM_CLASSES)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        levels = levels.to(DEVICE)\n",
    "        logits, probas = model(features)\n",
    "\n",
    "        #### CORAL loss\n",
    "        loss = coral_loss(logits, levels)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print(\n",
    "                \"Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f\"\n",
    "                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "\n",
    "def compute_mae_and_mse(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mae, mse, acc, num_examples = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits, probas = model(features)\n",
    "            predicted_labels = proba_to_label(probas).float()\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "            mse += torch.sum((predicted_labels - targets) ** 2)\n",
    "\n",
    "        mae = mae / num_examples\n",
    "        mse = mse / num_examples\n",
    "        return mae, mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\n",
    "test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (train/test): 0.92 | 0.85\n",
      "Mean squared error (train/test): 0.93 | 1.07\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}\")\n",
    "print(f\"Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Other\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on\", DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden_1=300, num_hidden_2=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.my_network = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            ### Specify CORN layer\n",
    "            torch.nn.Linear(50, (num_classes - 1))\n",
    "            ###--------------------------------------------------------------------###\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.my_network(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = MLP(input_size=50, num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/011 | Cost: 0.2115\n",
      "Epoch: 002/100 | Batch 000/011 | Cost: 0.1162\n",
      "Epoch: 003/100 | Batch 000/011 | Cost: 0.1362\n",
      "Epoch: 004/100 | Batch 000/011 | Cost: 0.1631\n",
      "Epoch: 005/100 | Batch 000/011 | Cost: 0.1534\n",
      "Epoch: 006/100 | Batch 000/011 | Cost: 0.1528\n",
      "Epoch: 007/100 | Batch 000/011 | Cost: 0.1057\n",
      "Epoch: 008/100 | Batch 000/011 | Cost: 0.1110\n",
      "Epoch: 009/100 | Batch 000/011 | Cost: 0.1342\n",
      "Epoch: 010/100 | Batch 000/011 | Cost: 0.0992\n",
      "Epoch: 011/100 | Batch 000/011 | Cost: 0.1270\n",
      "Epoch: 012/100 | Batch 000/011 | Cost: 0.0764\n",
      "Epoch: 013/100 | Batch 000/011 | Cost: 0.0928\n",
      "Epoch: 014/100 | Batch 000/011 | Cost: 0.0637\n",
      "Epoch: 015/100 | Batch 000/011 | Cost: 0.1006\n",
      "Epoch: 016/100 | Batch 000/011 | Cost: 0.0764\n",
      "Epoch: 017/100 | Batch 000/011 | Cost: 0.0971\n",
      "Epoch: 018/100 | Batch 000/011 | Cost: 0.1042\n",
      "Epoch: 019/100 | Batch 000/011 | Cost: 0.0603\n",
      "Epoch: 020/100 | Batch 000/011 | Cost: 0.0797\n",
      "Epoch: 021/100 | Batch 000/011 | Cost: 0.0916\n",
      "Epoch: 022/100 | Batch 000/011 | Cost: 0.0773\n",
      "Epoch: 023/100 | Batch 000/011 | Cost: 0.0727\n",
      "Epoch: 024/100 | Batch 000/011 | Cost: 0.0645\n",
      "Epoch: 025/100 | Batch 000/011 | Cost: 0.0683\n",
      "Epoch: 026/100 | Batch 000/011 | Cost: 0.0681\n",
      "Epoch: 027/100 | Batch 000/011 | Cost: 0.0520\n",
      "Epoch: 028/100 | Batch 000/011 | Cost: 0.0650\n",
      "Epoch: 029/100 | Batch 000/011 | Cost: 0.0579\n",
      "Epoch: 030/100 | Batch 000/011 | Cost: 0.0530\n",
      "Epoch: 031/100 | Batch 000/011 | Cost: 0.0601\n",
      "Epoch: 032/100 | Batch 000/011 | Cost: 0.0507\n",
      "Epoch: 033/100 | Batch 000/011 | Cost: 0.0536\n",
      "Epoch: 034/100 | Batch 000/011 | Cost: 0.0466\n",
      "Epoch: 035/100 | Batch 000/011 | Cost: 0.0706\n",
      "Epoch: 036/100 | Batch 000/011 | Cost: 0.0425\n",
      "Epoch: 037/100 | Batch 000/011 | Cost: 0.0316\n",
      "Epoch: 038/100 | Batch 000/011 | Cost: 0.0469\n",
      "Epoch: 039/100 | Batch 000/011 | Cost: 0.0400\n",
      "Epoch: 040/100 | Batch 000/011 | Cost: 0.0401\n",
      "Epoch: 041/100 | Batch 000/011 | Cost: 0.0440\n",
      "Epoch: 042/100 | Batch 000/011 | Cost: 0.0367\n",
      "Epoch: 043/100 | Batch 000/011 | Cost: 0.0469\n",
      "Epoch: 044/100 | Batch 000/011 | Cost: 0.0572\n",
      "Epoch: 045/100 | Batch 000/011 | Cost: 0.0414\n",
      "Epoch: 046/100 | Batch 000/011 | Cost: 0.0646\n",
      "Epoch: 047/100 | Batch 000/011 | Cost: 0.0346\n",
      "Epoch: 048/100 | Batch 000/011 | Cost: 0.0478\n",
      "Epoch: 049/100 | Batch 000/011 | Cost: 0.0484\n",
      "Epoch: 050/100 | Batch 000/011 | Cost: 0.0517\n",
      "Epoch: 051/100 | Batch 000/011 | Cost: 0.0239\n",
      "Epoch: 052/100 | Batch 000/011 | Cost: 0.0379\n",
      "Epoch: 053/100 | Batch 000/011 | Cost: 0.0586\n",
      "Epoch: 054/100 | Batch 000/011 | Cost: 0.0429\n",
      "Epoch: 055/100 | Batch 000/011 | Cost: 0.0425\n",
      "Epoch: 056/100 | Batch 000/011 | Cost: 0.0296\n",
      "Epoch: 057/100 | Batch 000/011 | Cost: 0.0432\n",
      "Epoch: 058/100 | Batch 000/011 | Cost: 0.0322\n",
      "Epoch: 059/100 | Batch 000/011 | Cost: 0.0352\n",
      "Epoch: 060/100 | Batch 000/011 | Cost: 0.0382\n",
      "Epoch: 061/100 | Batch 000/011 | Cost: 0.0515\n",
      "Epoch: 062/100 | Batch 000/011 | Cost: 0.0320\n",
      "Epoch: 063/100 | Batch 000/011 | Cost: 0.0283\n",
      "Epoch: 064/100 | Batch 000/011 | Cost: 0.0307\n",
      "Epoch: 065/100 | Batch 000/011 | Cost: 0.0317\n",
      "Epoch: 066/100 | Batch 000/011 | Cost: 0.0323\n",
      "Epoch: 067/100 | Batch 000/011 | Cost: 0.0351\n",
      "Epoch: 068/100 | Batch 000/011 | Cost: 0.0220\n",
      "Epoch: 069/100 | Batch 000/011 | Cost: 0.0355\n",
      "Epoch: 070/100 | Batch 000/011 | Cost: 0.0328\n",
      "Epoch: 071/100 | Batch 000/011 | Cost: 0.0431\n",
      "Epoch: 072/100 | Batch 000/011 | Cost: 0.0438\n",
      "Epoch: 073/100 | Batch 000/011 | Cost: 0.0342\n",
      "Epoch: 074/100 | Batch 000/011 | Cost: 0.0263\n",
      "Epoch: 075/100 | Batch 000/011 | Cost: 0.0314\n",
      "Epoch: 076/100 | Batch 000/011 | Cost: 0.0298\n",
      "Epoch: 077/100 | Batch 000/011 | Cost: 0.0232\n",
      "Epoch: 078/100 | Batch 000/011 | Cost: 0.0168\n",
      "Epoch: 079/100 | Batch 000/011 | Cost: 0.0261\n",
      "Epoch: 080/100 | Batch 000/011 | Cost: 0.0352\n",
      "Epoch: 081/100 | Batch 000/011 | Cost: 0.0237\n",
      "Epoch: 082/100 | Batch 000/011 | Cost: 0.0551\n",
      "Epoch: 083/100 | Batch 000/011 | Cost: 0.0213\n",
      "Epoch: 084/100 | Batch 000/011 | Cost: 0.0225\n",
      "Epoch: 085/100 | Batch 000/011 | Cost: 0.0329\n",
      "Epoch: 086/100 | Batch 000/011 | Cost: 0.0502\n",
      "Epoch: 087/100 | Batch 000/011 | Cost: 0.0455\n",
      "Epoch: 088/100 | Batch 000/011 | Cost: 0.0213\n",
      "Epoch: 089/100 | Batch 000/011 | Cost: 0.0347\n",
      "Epoch: 090/100 | Batch 000/011 | Cost: 0.0277\n",
      "Epoch: 091/100 | Batch 000/011 | Cost: 0.0347\n",
      "Epoch: 092/100 | Batch 000/011 | Cost: 0.0162\n",
      "Epoch: 093/100 | Batch 000/011 | Cost: 0.0258\n",
      "Epoch: 094/100 | Batch 000/011 | Cost: 0.0196\n",
      "Epoch: 095/100 | Batch 000/011 | Cost: 0.0243\n",
      "Epoch: 096/100 | Batch 000/011 | Cost: 0.0256\n",
      "Epoch: 097/100 | Batch 000/011 | Cost: 0.0214\n",
      "Epoch: 098/100 | Batch 000/011 | Cost: 0.0206\n",
      "Epoch: 099/100 | Batch 000/011 | Cost: 0.0195\n",
      "Epoch: 100/100 | Batch 000/011 | Cost: 0.0175\n"
     ]
    }
   ],
   "source": [
    "from coral_pytorch.losses import corn_loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        class_labels = class_labels.to(DEVICE)\n",
    "        features = features.to(DEVICE)\n",
    "        logits = model(features)\n",
    "\n",
    "        #### CORN loss\n",
    "        loss = corn_loss(logits, class_labels + 1, NUM_CLASSES)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print(\n",
    "                \"Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f\"\n",
    "                % (epoch + 1, num_epochs, batch_idx, len(train_loader), loss)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "\n",
    "\n",
    "def compute_mae_and_mse(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        mae, mse, acc, num_examples = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            predicted_labels = corn_label_from_logits(logits).float()\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "            mse += torch.sum((predicted_labels - targets) ** 2)\n",
    "\n",
    "        mae = mae / num_examples\n",
    "        mse = mse / num_examples\n",
    "        return mae, mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (train/test): 3.95 | 6.68\n",
      "Mean squared error (train/test): 34.77 | 69.69\n"
     ]
    }
   ],
   "source": [
    "train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\n",
    "test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n",
    "print(f\"Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}\")\n",
    "print(f\"Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
